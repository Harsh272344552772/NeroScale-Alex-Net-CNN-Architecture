{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6duUI0YdP37a"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install matplotlib numpy pillow opencv-python scikit-learn\n",
        "!pip install torchsummary tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SL8FAz_m0yJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3Nl13ECQQRs"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive to save/load models\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a directory for your project\n",
        "import os\n",
        "project_dir = '/content/drive/MyDrive/NeuroScale_AlexNet'\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "os.chdir(project_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEjqwD7aJawg"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuroScaleAlexPlusPlus(num_classes=10, input_channels=3)\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Model loaded on: {device}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SmrXZOdJoWE"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class NeuroScaleAlexPlusPlus(nn.Module):\n",
        "    def __init__(self, num_classes=10, input_channels=3):\n",
        "        super(NeuroScaleAlexPlusPlus, self).__init__()\n",
        "        # Define your model layers here\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1cDXoPfJukx"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Data preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
        "testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXN2Bif8Jxyq"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBmg-av0J1uJ"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(dataloader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Batch [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    return running_loss/len(dataloader), 100.*correct/total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPJbNaMQJ3wA",
        "outputId": "d14e7cd8-5a71-4fbe-e4dc-8fe47bf67314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [1/50]\n",
            "Batch [0/1563], Loss: 2.3004\n",
            "Batch [100/1563], Loss: 2.0884\n",
            "Batch [200/1563], Loss: 2.1748\n",
            "Batch [300/1563], Loss: 2.1798\n",
            "Batch [400/1563], Loss: 1.8987\n",
            "Batch [500/1563], Loss: 2.0095\n",
            "Batch [600/1563], Loss: 2.1949\n",
            "Batch [700/1563], Loss: 1.9490\n",
            "Batch [800/1563], Loss: 1.7234\n",
            "Batch [900/1563], Loss: 1.4179\n",
            "Batch [1000/1563], Loss: 1.7574\n",
            "Batch [1100/1563], Loss: 1.7020\n",
            "Batch [1200/1563], Loss: 1.6521\n",
            "Batch [1300/1563], Loss: 1.6787\n",
            "Batch [1400/1563], Loss: 1.7590\n",
            "Batch [1500/1563], Loss: 1.6594\n",
            "Train Loss: 1.8162, Train Acc: 31.66%\n",
            "\n",
            "Epoch [2/50]\n",
            "Batch [0/1563], Loss: 1.7761\n",
            "Batch [100/1563], Loss: 1.5811\n",
            "Batch [200/1563], Loss: 1.6108\n",
            "Batch [300/1563], Loss: 1.5535\n",
            "Batch [400/1563], Loss: 1.6834\n",
            "Batch [500/1563], Loss: 1.4400\n",
            "Batch [600/1563], Loss: 1.4250\n",
            "Batch [700/1563], Loss: 1.5035\n",
            "Batch [800/1563], Loss: 1.6691\n",
            "Batch [900/1563], Loss: 1.5451\n",
            "Batch [1000/1563], Loss: 1.8352\n",
            "Batch [1100/1563], Loss: 1.4901\n",
            "Batch [1200/1563], Loss: 1.5631\n",
            "Batch [1300/1563], Loss: 1.5689\n",
            "Batch [1400/1563], Loss: 1.5508\n",
            "Batch [1500/1563], Loss: 1.1603\n",
            "Train Loss: 1.5344, Train Acc: 43.59%\n",
            "\n",
            "Epoch [3/50]\n",
            "Batch [0/1563], Loss: 1.4304\n",
            "Batch [100/1563], Loss: 1.4734\n",
            "Batch [200/1563], Loss: 1.4244\n",
            "Batch [300/1563], Loss: 1.9519\n",
            "Batch [400/1563], Loss: 1.7629\n",
            "Batch [500/1563], Loss: 1.5180\n",
            "Batch [600/1563], Loss: 1.5381\n",
            "Batch [700/1563], Loss: 1.8604\n",
            "Batch [800/1563], Loss: 1.1629\n",
            "Batch [900/1563], Loss: 1.3415\n",
            "Batch [1000/1563], Loss: 1.4609\n",
            "Batch [1100/1563], Loss: 1.5520\n",
            "Batch [1200/1563], Loss: 1.4115\n",
            "Batch [1300/1563], Loss: 1.4872\n",
            "Batch [1400/1563], Loss: 1.6921\n",
            "Batch [1500/1563], Loss: 1.0455\n",
            "Train Loss: 1.4403, Train Acc: 47.41%\n",
            "\n",
            "Epoch [4/50]\n",
            "Batch [0/1563], Loss: 1.3090\n",
            "Batch [100/1563], Loss: 1.3137\n",
            "Batch [200/1563], Loss: 1.2925\n",
            "Batch [300/1563], Loss: 1.7200\n",
            "Batch [400/1563], Loss: 1.1942\n",
            "Batch [500/1563], Loss: 1.4734\n",
            "Batch [600/1563], Loss: 1.4937\n",
            "Batch [700/1563], Loss: 1.3563\n",
            "Batch [800/1563], Loss: 1.2624\n",
            "Batch [900/1563], Loss: 1.3881\n",
            "Batch [1000/1563], Loss: 1.5779\n",
            "Batch [1100/1563], Loss: 1.4963\n",
            "Batch [1200/1563], Loss: 1.8062\n",
            "Batch [1300/1563], Loss: 1.3788\n",
            "Batch [1400/1563], Loss: 1.2747\n",
            "Batch [1500/1563], Loss: 1.2946\n",
            "Train Loss: 1.3837, Train Acc: 49.93%\n",
            "\n",
            "Epoch [5/50]\n",
            "Batch [0/1563], Loss: 1.2871\n",
            "Batch [100/1563], Loss: 1.2612\n",
            "Batch [200/1563], Loss: 1.4855\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 50\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'\\nEpoch [{epoch+1}/{num_epochs}]')\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, trainloader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "\n",
        "    # Save checkpoint every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': train_loss,\n",
        "        }, f'checkpoint_epoch_{epoch+1}.pth')\n",
        "\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6YkZYrmJ_Sj"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def test_user_image():\n",
        "    # Upload image\n",
        "    print(\"Please upload an image:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        # Load and preprocess image\n",
        "        image = Image.open(filename).convert('RGB')\n",
        "\n",
        "        # Display original image\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(image)\n",
        "        plt.title('Original Image')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Preprocess for model\n",
        "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "        # Make prediction\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_tensor)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            predicted_class = torch.argmax(probabilities, dim=1)\n",
        "            confidence = torch.max(probabilities).item()\n",
        "\n",
        "        classes = [\n",
        "    'Healthy',\n",
        "    'Leaf Blight',\n",
        "    'Bacterial Spot',\n",
        "    'Early Blight',\n",
        "    'Late Blight',\n",
        "    'Leaf Curl',\n",
        "    'Powdery Mildew',\n",
        "    'Rust',\n",
        "    'Yellow Mosaic',\n",
        "    'Nutrient Deficiency'\n",
        "]\n",
        "\n",
        "\n",
        "        # Display results\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.bar(classes, probabilities[0].cpu().numpy())\n",
        "        plt.title('Class Probabilities')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.text(0.1, 0.8, f'Predicted: {classes[predicted_class.item()]}', fontsize=14)\n",
        "        plt.text(0.1, 0.6, f'Confidence: {confidence:.2%}', fontsize=14)\n",
        "        plt.text(0.1, 0.4, f'NeuroScale-Alex++Net Analysis', fontsize=12, style='italic')\n",
        "        plt.xlim(0, 1)\n",
        "        plt.ylim(0, 1)\n",
        "        plt.axis('off')\n",
        "        plt.title('Prediction Results')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return predicted_class.item(), confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wug3TED8KQ56"
      },
      "outputs": [],
      "source": [
        "test_user_image()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}